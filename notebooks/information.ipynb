{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Información  general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Datos del robot***\n",
    "\n",
    "base_link: 1\n",
    "\n",
    "* masa: 3.7\n",
    "* inercia: 0.0102675 0.0102675 0.00666\n",
    "\n",
    "shoulder_link: 2\n",
    "* masa: 8.393\n",
    "* inercia:  0.226891 0.226891 0.0151074\n",
    "\n",
    "elbow_link: 3\n",
    "* masa: 2.275\n",
    "* inercia: 0.0494433 0.0494433 0.004095\n",
    "\n",
    "wrist_1_link: 4\n",
    "* masa: 1.219\n",
    "* inercia: 0.21942 0.111173 0.111173\n",
    "\n",
    "wrist_2_link: 5\n",
    "* masa: 1.219\n",
    "* inercia:   0.21942 0.111173 0.111173\n",
    "\n",
    "wrist_3_link: 6\n",
    "* masa: 0.1879\n",
    "* inercia: 0.033822 0.0171365 0.0171365\n",
    "\n"
    "ee_link:\n",
    "* masa: 0.001\n",
    "* inercia: 1.66667e-08 1.66667e-08 1.66667e-08\n",
    "\n",
    "robotiq_85_base_link:\n",
    "* masa: 0.30915\n",
    "* inercia: 0.00030737 0.000289758 0.000199102\n",
    "\n",
    "left_outer_knuckle:\n",
    "* masa: 0.00684839\n",
    "* inercia: 1.38892e-06 1.28691e-06 2.45951e-07\n",
    "\n",
    "left_outer_finger:\n",
    "* masa: 0.0273094\n",
    "* inercia: 8.53972e-06 6.91333e-06 2.22664e-06\n",
    "\n",
    "left_inner_knuckle:\n",
    "* masa: 0.0110931\n",
    "* inercia: 5.59353e-06 3.96549e-06 1.88108e-06\n",
    "\n",
    "left_inner_finger:\n",
    "* masa: 0.00724255\n",
    "* inercia: 1.70064e-06 1.58577e-06 3.69621e-07\n",
    "\n",
    "right_inner_knuckle:\n",
    "* masa: 0.0110931\n",
    "* inercia: 5.59353e-06 3.96549e-06 1.88108e-06\n",
    "\n",
    "right_inner_finger:\n",
    "* masa: 0.00724255\n",
    "* inercia: 1.70064e-06 1.58577e-06 3.69621e-07\n",
    "\n",
    "\n",
    "right_outer_knuckle:\n",
    "* masa: 0.00684839\n",
    "* inercia: 1.38892e-06 1.28691e-06 2.45951e-07\n",
    "\n",
    "right_outer_finger:\n",
    "* masa: 0.0273094\n",
    "* inercia: 8.53972e-06 6.91333e-06 2.22664e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***articulaciones***\n",
    "\n",
    "TODO: agregar modelo del motor segun mujoco\n",
    "\n",
    "TIPO MOTOR  \n",
    "\n",
    "\n",
    "joint1:\n",
    "* rango de control: [-3.141 3.141] radianes\n",
    "* engranaje  o velocidad: 100\n",
    "\n",
    "joint2:\n",
    "* rango de control: [ 0 4.712]\n",
    "* engranaje o velocidad: 5\n",
    "\n",
    "joint3:\n",
    "* rango de control: [-3.141 3.141] radianes\n",
    "* engranaje  o velocidad: 5\n",
    "\n",
    "joint4:\n",
    "* rango de control: [-3.141 3.141] radianes\n",
    "* engranaje  o velocidad: 2\n",
    "\n",
    "joint5:\n",
    "* rango de control: [-3.141 3.141] radianes\n",
    "* engranaje  o velocidad: 2\n",
    "\n",
    "joint6:\n",
    "* rango de control: [-3.141 3.141] radianes\n",
    "* engranaje  o velocidad: 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Datos del ambiente***\n",
    "limites del target: x(-0.3, 0.1) y(-0.3, 0.3) z(0.45, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:MediumSeaGreen;\">Objetivos</h1>\n",
    "\n",
    "1. <h4 style=\"color:DodgerBlue;\">Determinar los requerimientos y prioridades de la empresa Bloomcker para el desarrollo de controladores inteligentes basados en aprendizaje por refuerzo.</h4>\n",
    "\n",
    "    En esta primera etapa, se procederá a investigar sobre las prioridades y requerimientos de la empresa Bloomcker, para el desarrollo de controladores inteligentes basados en aprendizaje por refuerzo, con el fin de poder establecer la información necesaria por parte de la empresa para poder adaptar el marco de trabajo al flujo de trabajo de la empresa Bloomcker.\n",
    "\n",
    "2. <h4 style=\"color:DodgerBlue;\">Documentar los conceptos básicos utilizados en aprendizaje por refuerzo para establecer una arquitectura de aprendizaje para un agente inteligente.</h4>\n",
    "\n",
    "    En esta segunda etapa, se procederá a realizar un levantamiento de información basado en la literatura, sobre los conceptos básicos que se utilizan para establecer una arquitectura de aprendizaje para un agente inteligente.\n",
    "\n",
    "\n",
    "3. <h4 style=\"color:DodgerBlue;\">Establecer criterios de diseño para la arquitectura de aprendizaje del agente inteligente implementado en un brazo manipulador.</h4>\n",
    "\n",
    "    En esta etapa, se establecerá un criterio de diseño para la arquitectura de aprendizaje que se quiera implementar basado en los requerimientos técnicos que caracterizan el sistema a tratar y como el agente tiene que aprender para lograr los objetivos de control del sistema. \n",
    "    \n",
    "\n",
    "4. <h4 style=\"color:DodgerBlue;\">Documentar los algoritmos básicos de aprendizaje por refuerzo más utilizados en controladores inteligentes implementados en brazos manipuladores.</h4>\n",
    "\n",
    "    Una vez obtenido los criterios de diseño, se documentará sobre los algoritmos de aprendizaje por refuerzo más utilizados en controladores inteligentes implementados en brazos manipuladores, para tener una referencia más clara y concisa sobre estos algoritmos al momento de seleccionar.\n",
    "\n",
    "5. <h4 style=\"color:DodgerBlue;\">Seleccionar las herramientas necesarias para el entrenamiento del agente inteligente.</h4>\n",
    "\n",
    "    En esta quinta etapa, se investigara sobre los diferentes tipos de librerías de software, herramientas de simulación y tipos de hardware mas utilizados para el desarrollo y entrenamiento de agentes inteligentes utilizando aprendizaje por refuerzo en brazos manipuladores. Luego de investigar estas herramientas, se seleccionara las que mejor se adapten para aplicarse en un brazo manipulador.\n",
    "\n",
    "6. <h4 style=\"color:DodgerBlue;\">Validar el marco de trabajo propuesto en un brazo manipulador simulado.</h4>\n",
    "\n",
    "    Para validar el marco de trabajo propuesto, se aplicará al caso de uso de un control de posición inteligente para el prototipo de un brazo manipulador simulado. Para lograr este objetivo, se hará uso de métricas y distintos parámetros que permitan supervisar y medir el desempeño del agente inteligente, con el fin de conseguir un margen de error lo más mínimo posible.\n",
    "    \n",
    "    Se caracterizara el tipo de brazo manipulador que se implementara en simulación para luego poder aplicar el marco de trabajo a este problema. Una vez escogido el algoritmo a implementar en el agente y diseñado la arquitectura de aprendizaje obtenida usando el marco de trabajo, se desarrollará utilizando la herramienta de software conveniente de simulación para el entorno de aprendizaje del agente inteligente, luego obtenido este entorno, se programará el algoritmo escogido en lenguaje de programación Python, en el agente inteligente para después ser implementado dentro del entorno de aprendizaje.\n",
    "    \n",
    "    Una vez terminado los pasos anteriores, se entrenará el agente en un hardware seleccionado. Al terminar el entrenamiento, se verificará si el agente logro realizar el controlador inteligente bajo los parámetros y métricas establecidos.\n",
    "\n",
    "7. <h4 style=\"color:DodgerBlue;\">Realizar un documento con los lineamientos para el diseño de controladores inteligentes basados en aprendizaje por refuerzo.</h4>\n",
    "\n",
    "\n",
    "    Con la información obtenida en las etapas anteriores, se recopilará toda la información para ser puesta en un documento con todos los lineamientos requeridos para el diseño de un controlador inteligente basado en aprendizaje por refuerzo.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:MediumSeaGreen;\">Dudas</h1>\n",
    "<ul>\n",
    "    <li>como me defiendo de preguntas de estabilidad</li>\n",
    "    <li>posibles preguntas de control</li>\n",
    "    <li>posibles preguntas de aplicación en un brazo real</li>\n",
    "    <li>discusión de criterios</li>\n",
    "    <li>capitulos del tomo</li>\n",
    "    <li>que resultados mostrar</li>\n",
    "</ul>\n",
    "\n",
    "hay que entregar:\n",
    "\n",
    "diagrama de entrenamiento\n",
    "\n",
    "diagrama de control\n",
    "\n",
    "descripción de la tarea de control, es decir el objetivo de control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:MediumSeaGreen;\">Entrenamiento de los diferentes agentes</h1>\n",
    "\n",
    "### agente Thursday \n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">Caracteristicas del ambiente</h4>\n",
    "\n",
    "* pasos de muestreo: 8ms ó 125Hz (simulation_frames = 4)\n",
    "* control del torque: 0.01 N/m\n",
    "* distancia minima para completar la tarea: 5cm\n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">caracteristicas del algoritmo</h4>\n",
    "\n",
    "* algoritmo ddpg\n",
    "* función de activación de la NN: ReLU\n",
    "* tamaño de la politica: 32x32\n",
    "* tamaño del episodio: 4 seg (max_ep_len=500)\n",
    "* tiempo de entrenamiento durado: 1:13h tomando en cuenta la gravedad, 75min sin gravedad\n",
    "\n",
    "<h3>Notas del entrenamiento</h3>\n",
    "\n",
    "No funciona el entrenamiento tomando en cuenta la gravedad ó sin ella. se recomienda aumentar el numero de episodios, sin embargo en la epoca 86 el reward por episodio empieza a aumentar negativamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agente sunday\n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">Caracteristicas del ambiente</h4>\n",
    "\n",
    "* pasos de muestreo: 8ms ó 125Hz (simulation_frames = 4)\n",
    "* control del torque: 0.01 N/m\n",
    "* distancia minima para completar la tarea: 5cm\n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">caracteristicas del algoritmo</h4>\n",
    "\n",
    "* <p style=\"\">algoritmo ddpg</p>\n",
    "* función de activación de la NN: ReLU\n",
    "* tamaño de la politica: 400x300\n",
    "* tamaño del episodio: 4 seg (max_ep_len=500)\n",
    "\n",
    "<h4 style=\"color:DodgerBlue\"> notas </h4>\n",
    "\n",
    "el agente 1 se entreno sin gravedad y funciono a la perfección, al agregarle gravedad podia realizar la tarea pero con peor rendimiento. agent1 es el entrenado con gravedad, y agents1 sin gravedad. el entrenamiento duro 376.55 minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agentes\n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">ddpg1</h4>\n",
    "\n",
    "Entrenado sin gravedad duro 376.55 minutos,  politica de 400x300, 150 epocas\n",
    "\n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">ddpg2</h4>\n",
    "\n",
    "Entrenado sin gravedad y para otro target duro 14h, entrenado sin gravedad, 150 epocas\n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">ddpg3</h4>\n",
    "\n",
    "Entrenado con gravedad, politica de 400x300, y 120 epocas,\n",
    "\n",
    "<h4 style=\"color:DodgerBlue;\">ddpg4</h4>\n",
    "\n",
    "Entrenado sin gravedad con politica de 32x32 y 100 epocas\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
